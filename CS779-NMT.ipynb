{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13431808,"sourceType":"datasetVersion","datasetId":8525257}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport json\nimport numpy as np\nimport re\nimport pandas as pd\nimport heapq\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom google.colab import drive","metadata":{"id":"lhLKQGTnuK8y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nwith open('/kaggle/input/new-data/train_data1.json', 'r') as file:\n    data_train = json.load(file)\n\nwith open('/kaggle/input/new-data/val_data1.json', 'r') as file:\n    data_val = json.load(file)","metadata":{"id":"_Wq9LVApuS_h","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_language_data_train(data, language_pair):\n    \"\"\"\n    Extracts train and validation sentences and IDs for a given language pair.\n\n    Args:\n        data (dict): Nested dictionary containing all language pairs.\n        language_pair (str): The language pair to extract (\"English-Bengali\", \"English-Hindi\", etc.)\n\n    Returns:\n        tuple: (source_train, target_train, train_ids)\n    \"\"\"\n    source_lst, target_lst, ids_lst = [], [], []\n\n    for lp, lp_data in data.items():\n        if lp == language_pair:\n            for data_type, data_entries in lp_data.items():\n                for entry_id, entry_data in data_entries.items():\n                    source = entry_data[\"source\"]\n                    target = entry_data[\"target\"]\n\n                    source_lst.append(source)\n                    target_lst.append(target)\n                    ids_lst.append(entry_id)\n\n    return source_lst, target_lst, ids_lst\n\n","metadata":{"id":"sVMigPDfuWlk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_language_data_val(data , language_pair):\n\n  source_lst, ids_lst = [], []\n\n  for lp, lp_data in data.items():\n      if lp == language_pair:\n          for data_type, data_entries in lp_data.items():\n              for entry_id, entry_data in data_entries.items():\n                  source = entry_data[\"source\"]\n\n\n                  source_lst.append(source)\n\n                  ids_lst.append(entry_id)\n\n  return source_lst, ids_lst\n\n\n","metadata":{"id":"Ftil_iea6vQb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source_train_ben, target_train_ben , train_ids_ben  = extract_language_data_train(data_train, \"English-Bengali\")\nsource_val_ben, val_ids_ben = extract_language_data_val(data_val, \"English-Bengali\")\n\nsource_train_hin, target_train_hin, train_ids_hin = extract_language_data_train(data_train, \"English-Hindi\")\nsource_val_hin, val_ids_hin = extract_language_data_val(data_val, \"English-Hindi\")\n","metadata":{"id":"4DpKujpk3cu2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Node:\n    def __init__(self, token_id):\n        self.token_id = token_id\n        self.prev = None\n        self.next = None\n\nclass BPETokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.inverse_vocab = {}\n        self.merges = {}\n        self.token_to_id = {}\n        self.id_to_token = {}\n\n    def initialize_vocab(self):\n        reserved = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n        self.vocab = {i: tok for i, tok in enumerate(reserved)}\n        self.inverse_vocab = {tok: i for i, tok in self.vocab.items()}\n        return len(self.vocab)\n\n    def build_corpus(self, text, next_token_id):\n        words = [list(w) + [\"</w>\"] for w in text.strip().split()]\n        corpus = []\n        for w in words:\n            head, prev = None, None\n            for ch in w:\n                if ch not in self.inverse_vocab:\n                    self.vocab[next_token_id] = ch\n                    self.inverse_vocab[ch] = next_token_id\n                    next_token_id += 1\n                node = Node(self.inverse_vocab[ch])\n                if prev:\n                    prev.next, node.prev = node, prev\n                else:\n                    head = node\n                prev = node\n            corpus.append(head)\n        return corpus, next_token_id\n\n    def count_pairs(self, corpus):\n        pair_occurrences = defaultdict(set)\n        for head in corpus:\n            node = head\n            while node and node.next:\n                pair_occurrences[(node.token_id, node.next.token_id)].add(node)\n                node = node.next\n        return pair_occurrences\n\n    def merge_pair(self, pair, new_id, pair_occurrences, heap):\n        t1, t2 = pair\n        new_tok = self.vocab[t1] + self.vocab[t2]\n        self.vocab[new_id] = new_tok\n        self.inverse_vocab[new_tok] = new_id\n        affected = list(pair_occurrences[pair])\n        pair_occurrences[pair].clear()\n        for node in affected:\n            if not node.next or node.token_id != t1 or node.next.token_id != t2:\n                continue\n            node.token_id = new_id\n            removed = node.next\n            node.next = removed.next\n            if removed.next:\n                removed.next.prev = node\n            if node.prev:\n                old = (node.prev.token_id, t1)\n                pair_occurrences[old].discard(node.prev)\n                new = (node.prev.token_id, node.token_id)\n                pair_occurrences[new].add(node.prev)\n                heapq.heappush(heap, (-len(pair_occurrences[new]), new))\n            if node.next:\n                old = (t2, node.next.token_id)\n                pair_occurrences[old].discard(node)\n                new = (node.token_id, node.next.token_id)\n                pair_occurrences[new].add(node)\n                heapq.heappush(heap, (-len(pair_occurrences[new]), new))\n\n    def train(self, text, vocab_size=5000):\n        next_id = self.initialize_vocab()\n        corpus, next_id = self.build_corpus(text, next_id)\n        pair_occurrences = self.count_pairs(corpus)\n        heap = [(-len(nodes), pair) for pair, nodes in pair_occurrences.items()]\n        heapq.heapify(heap)\n        while len(self.vocab) < vocab_size and heap:\n            freq, pair = heapq.heappop(heap)\n            freq = -freq\n            if freq == 0 or len(pair_occurrences[pair]) != freq:\n                continue\n            self.merges[pair] = next_id\n            self.merge_pair(pair, next_id, pair_occurrences, heap)\n            next_id += 1\n        self.token_to_id = {tok: tid for tid, tok in self.vocab.items()}\n        self.id_to_token = {tid: tok for tid, tok in self.vocab.items()}\n\n    def tokenize(self, text):\n        words = [list(w) + [\"</w>\"] for w in text.strip().split()]\n        tokens = []\n        for w in words:\n            ids = [self.inverse_vocab.get(ch, self.token_to_id[\"<UNK>\"]) for ch in w]\n            merged = True\n            while merged:\n                merged, i = False, 0\n                while i < len(ids) - 1:\n                    pair = (ids[i], ids[i + 1])\n                    if pair in self.merges:\n                        ids[i] = self.merges[pair]\n                        ids.pop(i + 1)\n                        merged = True\n                    else:\n                        i += 1\n            tokens.extend(ids)\n        return tokens\n\n\n    def decode(self, token_ids):\n        specials = {self.token_to_id.get(\"<PAD>\"), self.token_to_id.get(\"<UNK>\"),\n                    self.token_to_id.get(\"<SOS>\"), self.token_to_id.get(\"<EOS>\")}\n        words, cur = [], []\n        for tid in token_ids:\n            if tid in specials:\n                continue\n            tok = self.id_to_token.get(tid, \"<UNK>\")\n            if tok.endswith(\"</w>\"):\n                cur.append(tok[:-4])\n                if cur:\n                    words.append(\"\".join(cur))\n                cur = []\n            else:\n                cur.append(tok)\n        if cur:\n            words.append(\"\".join(cur))\n        return \" \".join(words).strip()\n","metadata":{"id":"1uC8f9WpuZqy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text, lang=\"en\"):\n    \"\"\"Cleans and normalizes text before tokenization.\"\"\"\n    text = text.strip()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[“”‘’]', '\"', text)\n    if lang == \"hi\":\n        text = re.sub(r'[^\\u0900-\\u097F\\s.,!?]', '', text)\n    elif lang == \"bn\":\n        text = re.sub(r'[^\\u0980-\\u09FF\\s.,!?।]', '', text)\n    else:\n        text = re.sub(r'[^\\w\\s.,!?]', '', text)\n        text = text.lower()\n    return text","metadata":{"id":"HPghiLl3Abkv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_ben_bpe = BPETokenizer()\neng_ben_bpe.train(\" \".join([preprocess_text(s, \"en\") for s in source_train_ben]), vocab_size=20000)\n\nben_bpe = BPETokenizer()\nben_bpe.train(\" \".join([preprocess_text(s, \"bn\") for s in target_train_ben]), vocab_size=20000)","metadata":{"id":"CH25hMMEApEq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bengali_text = \"এই জায়গাগুলো দেখতে ভুলো না।\"\ntokens = ben_bpe.tokenize(bengali_text)\ndecoded_sentence = ben_bpe.decode(tokens)\nprint(decoded_sentence)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zihyhs3d75CY","outputId":"c13ad82a-7016-487c-ebb4-85a34b152eb1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_hin_bpe = BPETokenizer()\neng_hin_bpe.train(\" \".join([preprocess_text(s, \"en\") for s in source_train_hin]), vocab_size=20000)\n\nhin_bpe = BPETokenizer()\nhin_bpe.train(\" \".join([preprocess_text(s, \"hi\") for s in target_train_hin]), vocab_size=20000)","metadata":{"id":"0TMnO5CrA3IR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hindi_text = \"इन जगहों को देखना मत भूलना।\"\ntokens = hin_bpe.tokenize(hindi_text)\nprint(\"Tokens:\", tokens)\ndecoded_sentence = hin_bpe.decode(tokens)\nprint(\"Decoded Sentence:\", decoded_sentence)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsweF7c387P6","outputId":"d94d34b5-8619-452b-db83-b89a89f77e4c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for tokenizer in [ben_bpe, hin_bpe]:\n    for t in [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n        if t not in tokenizer.token_to_id:\n            new_id = len(tokenizer.token_to_id)\n            tokenizer.token_to_id[t] = new_id\n            tokenizer.id_to_token[new_id] = t","metadata":{"id":"aTvO-lf7Y_iV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seq_length = 30\n\ndef encode_and_pad(tokenizer, sent, max_length):\n    sos = [tokenizer.token_to_id[\"<SOS>\"]]\n    eos = [tokenizer.token_to_id[\"<EOS>\"]]\n    pad = [tokenizer.token_to_id[\"<PAD>\"]]\n    encoded = tokenizer.tokenize(sent)\n    if len(encoded) < max_length - 2:\n        n_pads = max_length - 2 - len(encoded)\n        return sos + encoded + eos + pad * n_pads\n    else:\n        return sos + encoded[:max_length - 2] + eos","metadata":{"id":"_gwPj_JsuwMA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_train_encoded_ben = [encode_and_pad(eng_ben_bpe, sent, seq_length) for sent in source_train_ben]\nde_train_encoded_ben = [encode_and_pad(ben_bpe, sent, seq_length) for sent in target_train_ben]\nen_val_encoded_ben = [encode_and_pad(eng_ben_bpe, sent, seq_length) for sent in source_val_ben]\nde_val_encoded_ben = [encode_and_pad(ben_bpe, sent, seq_length) for sent in source_val_ben]\n\n\nen_train_encoded_hin = [encode_and_pad(eng_hin_bpe, sent, seq_length) for sent in source_train_hin]\nde_train_encoded_hin = [encode_and_pad(hin_bpe, sent, seq_length) for sent in target_train_hin]\nen_val_encoded_hin = [encode_and_pad(eng_hin_bpe, sent, seq_length) for sent in source_val_hin]\nde_val_encoded_hin = [encode_and_pad(hin_bpe, sent, seq_length) for sent in source_val_hin]\n\n\n\ntrain_x_ben = np.array(en_train_encoded_ben)\ntrain_y_ben = np.array(de_train_encoded_ben)\ntest_x_ben = np.array(en_val_encoded_ben)\ntest_y_ben = np.array(de_val_encoded_ben)\n\ntrain_x_hin = np.array(en_train_encoded_hin)\ntrain_y_hin = np.array(de_train_encoded_hin)\ntest_x_hin = np.array(en_val_encoded_hin)\ntest_y_hin = np.array(de_val_encoded_hin)\n\nbatch_size =100\n\ntrain_ds_ben = TensorDataset(torch.from_numpy(train_x_ben), torch.from_numpy(train_y_ben))\ntest_ds_ben = TensorDataset(torch.from_numpy(test_x_ben), torch.from_numpy(test_y_ben))\ntrain_dl_ben = DataLoader(train_ds_ben, shuffle=True, batch_size=batch_size, drop_last=True)\ntest_dl_ben = DataLoader(test_ds_ben, shuffle=False, batch_size=batch_size)\n\n\ntrain_ds_hin = TensorDataset(torch.from_numpy(train_x_hin), torch.from_numpy(train_y_hin))\ntest_ds_hin = TensorDataset(torch.from_numpy(test_x_hin), torch.from_numpy(test_y_hin))\ntrain_dl_hin = DataLoader(train_ds_hin, shuffle=True, batch_size=batch_size, drop_last=True)\ntest_dl_hin = DataLoader(test_ds_hin, shuffle=False, batch_size=batch_size)","metadata":{"id":"eL7W8--JuhWR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # import torch\n# # import torch.nn as nn\n# # import torch.optim as optim\n# # import numpy as np\n# # import pandas as pd\n\n# # class EncoderRNN(nn.Module):\n# #     def __init__(self, input_size, hidden_size):\n# #         super(EncoderRNN, self).__init__()\n# #         self.hidden_size = hidden_size\n# #         self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=0)\n# #         self.dropout = nn.Dropout(0.3)  # CHANGE: Added dropout for regularization\n# #         self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2, batch_first=True, dropout=0.3)  # CHANGE: 2-layer GRU\n\n# #     def forward(self, x, hidden):\n# #         embedded = self.dropout(self.embedding(x))\n# #         output, hidden = self.gru(embedded, hidden)\n# #         return output, hidden\n\n# #     def init_hidden(self, batch_size, device):\n# #         return torch.zeros(2, batch_size, self.hidden_size, device=device)  # CHANGE: num_layers=2\n\n\n# # class DecoderRNN(nn.Module):\n# #     def __init__(self, hidden_size, output_size):\n# #         super(DecoderRNN, self).__init__()\n# #         self.hidden_size = hidden_size\n# #         self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n# #         self.dropout = nn.Dropout(0.3)  \n# #         self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2, batch_first=True, dropout=0.3)  # CHANGE\n# #         self.fc = nn.Linear(hidden_size, output_size)\n\n# #     def forward(self, x, hidden):\n# #         embedded = self.dropout(self.embedding(x))\n# #         output, hidden = self.gru(embedded, hidden)\n# #         output = self.fc(output)\n# #         return output, hidden\n\n\n\n# # def train_model(encoder, decoder, train_dl, tokenizer, epochs, lr=0.0025, teacher_forcing_ratio=0.8):  #CHANGE: lower lr\n# #     print(\"training started : \")\n\n# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# #     print(device)\n# #     encoder.to(device)\n# #     decoder.to(device)\n# #     criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id[\"<PAD>\"])\n# #     enc_opt = optim.AdamW(encoder.parameters(), lr=lr, weight_decay=1e-4)  #CHANGE: AdamW optimizer\n# #     dec_opt = optim.AdamW(decoder.parameters(), lr=lr, weight_decay=1e-4)\n# #     scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, 'min', patience=2, factor=0.5)  #CHANGE: LR scheduler\n# #     # criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id[\"<PAD>\"])\n# #     # enc_opt = optim.Adagrad(encoder.parameters(), lr=lr, weight_decay=1e-5)\n# #     # dec_opt = optim.Adagrad(decoder.parameters(), lr=lr, weight_decay=1e-5)\n# #     # scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, 'min', patience=3, factor=0.5)\n\n\n# #     for epoch in range(epochs):\n# #         encoder.train()\n# #         decoder.train()\n# #         total_loss = 0\n\n# #         for src, tgt in train_dl:\n# #             batch_size = src.size(0)\n# #             src, tgt = src.to(device), tgt.to(device)\n# #             enc_hidden = encoder.init_hidden(batch_size, device)\n\n# #             enc_opt.zero_grad()\n# #             dec_opt.zero_grad()\n\n# #             # Encode\n# #             _, hidden = encoder(src, enc_hidden)\n\n# #             # Initialize decoder input with <SOS>\n# #             dec_input = tgt[:, 0].unsqueeze(1)\n# #             dec_hidden = hidden\n# #             outputs = []\n\n# #             # Step-by-step decoding\n# #             for t in range(1, tgt.size(1)):\n# #                 out, dec_hidden = decoder(dec_input, dec_hidden)\n# #                 pred = out[:, -1, :]\n# #                 outputs.append(pred.unsqueeze(1))\n\n# #                 teacher_force = np.random.rand() < teacher_forcing_ratio\n# #                 next_input = tgt[:, t] if teacher_force else pred.argmax(1)\n# #                 dec_input = next_input.unsqueeze(1)\n\n# #             outputs = torch.cat(outputs, dim=1)\n# #             loss = criterion(outputs.reshape(-1, outputs.shape[-1]), tgt[:, 1:].reshape(-1))\n# #             loss.backward()\n\n# #             # CHANGE: Gradient clipping for stability\n# #             torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n# #             torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n# #             enc_opt.step()\n# #             dec_opt.step()\n\n# #             total_loss += loss.item()\n\n# #         avg_loss = total_loss / len(train_dl)\n# #         scheduler.step(avg_loss)  # CHANGE: adjust learning rate dynamically\n# #         print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {enc_opt.param_groups[0]['lr']:.6f}\")\n\n\n# # def translate(encoder, decoder, dataloader, tokenizer, seq_length=40):\n# #     encoder.eval()\n# #     decoder.eval()\n# #     device = next(encoder.parameters()).device\n# #     SOS = tokenizer.token_to_id[\"<SOS>\"]\n# #     EOS = tokenizer.token_to_id[\"<EOS>\"]\n# #     preds = []\n\n# #     with torch.no_grad():\n# #         for src, _ in dataloader:\n# #             batch_size = src.size(0)\n# #             src = src.to(device)\n# #             enc_hidden = encoder.init_hidden(batch_size, device)\n# #             _, hidden = encoder(src, enc_hidden)\n\n# #             dec_input = torch.full((batch_size, 1), SOS, dtype=torch.long, device=device)\n# #             dec_hidden = hidden\n# #             seq_preds = []\n\n# #             for _ in range(seq_length):\n# #                 dec_out, dec_hidden = decoder(dec_input, dec_hidden)\n# #                 dec_out = dec_out.argmax(-1)\n# #                 seq_preds.append(dec_out)\n# #                 dec_input = dec_out\n\n# #             seq_preds = torch.stack(seq_preds, dim=1)\n# #             for s in seq_preds:\n# #                ids = [i.item() for i in s if i.item() not in (SOS, EOS, tokenizer.token_to_id[\"<PAD>\"])]\n# #                preds.append(tokenizer.decode(ids))\n# #     return preds\n\n\n# # hidden_size = 256\n# # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # encoder_input_size_ben = max(np.max(en_train_encoded_ben), np.max(en_val_encoded_ben)) + 1\n# # decoder_output_size_ben = max(np.max(de_train_encoded_ben), np.max(de_val_encoded_ben)) + 1\n\n# # encoder_ben = EncoderRNN(encoder_input_size_ben, hidden_size).to(device)\n# # decoder_ben = DecoderRNN(hidden_size, decoder_output_size_ben).to(device)\n\n# # print(\"Training Bengali model...\")\n# # train_model(encoder_ben, decoder_ben, train_dl_ben, ben_bpe, epochs=5, lr=0.001, teacher_forcing_ratio=0.6)  #CHANGE: longer training, higher TF ratio\n\n\n\n# # encoder_input_size_hin = max(np.max(en_train_encoded_hin), np.max(en_val_encoded_hin)) + 1\n# # decoder_output_size_hin = max(np.max(de_train_encoded_hin), np.max(de_val_encoded_hin)) + 1\n\n# # encoder_hin = EncoderRNN(encoder_input_size_hin, hidden_size).to(device)\n# # decoder_hin = DecoderRNN(hidden_size, decoder_output_size_hin).to(device)\n\n# # print(\"\\nTraining Hindi model...\")\n# # train_model(encoder_hin, decoder_hin, train_dl_hin, hin_bpe, epochs=5, lr=0.001, teacher_forcing_ratio=0.6)  #CHANGE\n\n\n\n# # print(\"\\nGenerating Bengali translations...\")\n# # val_outs_ben = translate(encoder_ben, decoder_ben, test_dl_ben, ben_bpe)\n# # df_ben = pd.DataFrame({\"ID\": val_ids_ben, \"Translation\": val_outs_ben})\n# # df_ben.to_csv(\"answer_ben.csv\", index=False)\n# # print(\"Saved Bengali predictions → answer_ben.csv\")\n\n# # print(\"\\nGenerating Hindi translations...\")\n# # val_outs_hin = translate(encoder_hin, decoder_hin, test_dl_hin, hin_bpe)\n# # df_hin = pd.DataFrame({\"ID\": val_ids_hin, \"Translation\": val_outs_hin})\n# # df_hin.to_csv(\"answer_hi.csv\", index=False)\n# # print(\"Saved Hindi predictions → answer_hi.csv\")\n\n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import numpy as np\n# import pandas as pd\n\n\n\n# class EncoderRNN(nn.Module):\n#     def __init__(self, input_size, hidden_size=512, num_layers=2, emb_dropout=0.2, rnn_dropout=0.2):\n#         super(EncoderRNN, self).__init__()\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n\n#         self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=0)\n#         self.dropout = nn.Dropout(emb_dropout)\n#         self.lstm = nn.LSTM(\n#             input_size=hidden_size,\n#             hidden_size=hidden_size,\n#             num_layers=num_layers,\n#             batch_first=True,\n#             dropout=rnn_dropout if num_layers > 1 else 0.0\n#         )\n\n#     def forward(self, x, hidden):\n#         \"\"\"\n#         x: (B, T) long\n#         hidden:  LSTM (h0, c0)\n#         returns:\n#           outputs: (B, T, H)\n#           hidden: (hT, cT)\n#         \"\"\"\n#         B = x.size(0)\n#         device = x.device\n\n#         h0, c0 = self._to_lstm_hidden(hidden, B, device)\n#         emb = self.dropout(self.embedding(x))  # (B,T,H)\n#         outputs, (hT, cT) = self.lstm(emb, (h0, c0))\n#         return outputs, (hT, cT)\n\n#     def init_hidden(self, batch_size, device):\n#         \"\"\"\n#         Backward-compatible: returns GRU-style h0 (num_layers,B,H).\n#         We create c0 internally in forward().\n#         \"\"\"\n#         return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n\n#     def _to_lstm_hidden(self, hidden, batch_size, device):\n#         if isinstance(hidden, tuple):\n#             h0, c0 = hidden\n#         else:\n#             h0 = hidden\n#             c0 = torch.zeros_like(h0, device=device)\n#         return h0, c0\n\n\n# class DecoderRNN(nn.Module):\n#     def __init__(self, hidden_size=512, output_size=None, num_layers=2, emb_dropout=0.3, rnn_dropout=0.3):\n#         super(DecoderRNN, self).__init__()\n#         assert output_size is not None, \"output_size (target vocab size)\"\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n#         self.output_size = output_size\n\n#         self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n#         self.dropout = nn.Dropout(emb_dropout)\n#         self.lstm = nn.LSTM(\n#             input_size=hidden_size,\n#             hidden_size=hidden_size,\n#             num_layers=num_layers,\n#             batch_first=True,\n#             dropout=rnn_dropout if num_layers > 1 else 0.0\n#         )\n#         self.fc = nn.Linear(hidden_size, output_size)\n\n#     def forward(self, x, hidden):\n#         \"\"\"\n#         x: (B,T) or (B,) or (1,B)\n#         hidden: LSTM (h,c)\n#         returns:\n#           logits: (B,T,V) or (B,V) for single-step\n#           hidden: (hT, cT)\n#         \"\"\"\n#         x, squeeze_back = self._normalize_input_shape(x)\n#         B = x.size(0)\n#         device = x.device\n\n#         h0, c0 = self._to_lstm_hidden(hidden, B, device)\n\n#         emb = self.dropout(self.embedding(x))        # (B,T,H)\n#         out, (hT, cT) = self.lstm(emb, (h0, c0))     # (B,T,H)\n#         logits = self.fc(out)                        # (B,T,V)\n\n#         if squeeze_back:\n#             logits = logits.squeeze(1)               # (B,V)\n#         return logits, (hT, cT)\n\n#     def _normalize_input_shape(self, x):\n#         squeeze_back = False\n#         if x.dim() == 1:\n#             x = x.unsqueeze(1)         # (B,) -> (B,1)\n#             squeeze_back = True\n#         elif x.dim() == 2 and x.size(0) == 1:\n#             x = x.transpose(0, 1)      # (1,B) -> (B,1)\n#             squeeze_back = True\n#         return x, squeeze_back\n\n#     def _to_lstm_hidden(self, hidden, batch_size, device):\n#         if isinstance(hidden, tuple):\n#             return hidden\n#         else:\n#             h = hidden\n#             c = torch.zeros_like(h, device=device)\n#             return (h, c)\n\n\n# def train_model(encoder, decoder, train_dl, tokenizer, epochs=600, lr=0.0001, teacher_forcing_ratio=0.78):\n#     \"\"\"\n#     train_dl yield: (src, tgt)\n#       - src: (B, T_src) LongTensor\n#       - tgt: (B, T_tgt) LongTensor with tgt[:,0] == <SOS>\n#     tokenizer: used only to get PAD id for loss ignore\n#     \"\"\"\n#     print(\"training started : \")\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     # print(\"device:\", device)\n\n#     encoder.to(device)\n#     decoder.to(device)\n\n#     pad_id = tokenizer.token_to_id[\"<PAD>\"]\n#     criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n\n#     enc_opt = optim.AdamW(encoder.parameters(), lr=lr, weight_decay=1e-4)\n#     dec_opt = optim.AdamW(decoder.parameters(), lr=lr, weight_decay=1e-4)\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, mode='min', patience=2, factor=0.5)\n\n#     for epoch in range(epochs):\n#         encoder.train()\n#         decoder.train()\n#         total_loss = 0.0\n\n#         for src, tgt in train_dl:\n#             src, tgt = src.to(device), tgt.to(device)\n#             batch_size = src.size(0)\n\n#             # init encoder hidden (GRU-style h0; c0 is created internally)\n#             enc_hidden = encoder.init_hidden(batch_size, device)\n\n#             enc_opt.zero_grad(set_to_none=True)\n#             dec_opt.zero_grad(set_to_none=True)\n\n#             # Encode\n#             _, enc_last = encoder(src, enc_hidden)    # enc_last is (hT, cT)\n\n#             # Decode with teacher forcing\n#             dec_hidden = enc_last\n#             dec_input = tgt[:, 0]                     # (B,) <SOS>\n#             outputs = []\n\n#             for t in range(1, tgt.size(1)):\n#                 # Decoder step: returns logits (B,V) for single-step input\n#                 step_logits, dec_hidden = decoder(dec_input, dec_hidden)   # (B,V)\n#                 outputs.append(step_logits.unsqueeze(1))                   # (B,1,V)\n\n#                 teacher_force = (np.random.rand() < teacher_forcing_ratio)\n#                 next_input = tgt[:, t] if teacher_force else step_logits.argmax(-1)\n#                 dec_input = next_input\n\n#             outputs = torch.cat(outputs, dim=1)       # (B, T_tgt-1, V)\n#             loss = criterion(\n#                 outputs.reshape(-1, outputs.size(-1)),   # (B*(T-1), V)\n#                 tgt[:, 1:].reshape(-1)                   # (B*(T-1),)\n#             )\n\n#             loss.backward()\n#             torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n#             torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1.0)\n#             enc_opt.step()\n#             dec_opt.step()\n\n#             total_loss += float(loss.item())\n\n#         avg_loss = total_loss / max(1, len(train_dl))\n#         scheduler.step(avg_loss)\n#         print(f\"Epoch {epoch+1}/{epochs} | loss: {avg_loss:.4f} | lr: {enc_opt.param_groups[0]['lr']:.6f}\")\n\n\n# @torch.no_grad()\n# def translate(encoder, decoder, dataloader, tokenizer, seq_length=40):\n#     encoder.eval()\n#     decoder.eval()\n#     device = next(encoder.parameters()).device\n\n#     SOS = tokenizer.token_to_id[\"<SOS>\"]\n#     EOS = tokenizer.token_to_id[\"<EOS>\"]\n#     PAD = tokenizer.token_to_id[\"<PAD>\"]\n\n#     preds = []\n#     for src, _ in dataloader:\n#         src = src.to(device)\n#         B = src.size(0)\n\n#         enc_hidden = encoder.init_hidden(B, device)\n#         _, enc_last = encoder(src, enc_hidden)\n\n#         dec_hidden = enc_last\n#         dec_input = torch.full((B,), SOS, dtype=torch.long, device=device)\n\n#         seq_preds = []\n#         for _ in range(seq_length):\n#             step_logits, dec_hidden = decoder(dec_input, dec_hidden)  # (B,V)\n#             next_ids = step_logits.argmax(-1)                         # (B,)\n#             seq_preds.append(next_ids.unsqueeze(1))                   # (B,1)\n#             dec_input = next_ids\n\n#         seq_preds = torch.cat(seq_preds, dim=1)  # (B, seq_length)\n\n#         # detokenize each sequence\n#         for s in seq_preds:\n#             ids = [i.item() for i in s if i.item() not in (SOS, EOS, PAD)]\n#             preds.append(tokenizer.decode(ids))\n#     return preds\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\n\n# ======================\n#  BiLSTM Encoder (no extra methods; only __init__ and forward)\n# ======================\n\nclass EncoderRNN(nn.Module):\n    \"\"\"\n    Bidirectional LSTM encoder.\n    - input_size: source vocab size\n    - hidden_size: 'model size' (decoder hidden). Encoder uses hidden_size//2 per direction.\n    \"\"\"\n    def __init__(self, input_size, hidden_size=512, num_layers=2,\n                 emb_dropout=0.2, rnn_dropout=0.2, padding_idx=0):\n        super().__init__()\n        assert hidden_size % 2 == 0, \"hidden_size must be even for bidirectional encoder.\"\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.h_per_dir = hidden_size // 2  # concat -> hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=padding_idx)\n        self.dropout   = nn.Dropout(emb_dropout)\n        self.rnn = nn.LSTM(\n            input_size  = hidden_size,\n            hidden_size = self.h_per_dir,\n            num_layers  = num_layers,\n            batch_first = True,\n            bidirectional = True,\n            dropout = rnn_dropout if num_layers > 1 else 0.0,\n        )\n\n        # Bridge: map final bi states -> decoder initial (per layer).\n        # We fuse last-layer forward/backward (cat along dim=1) -> (B, hidden_size)\n        # and project to (B, hidden_size) for h and c, then repeat for num_layers.\n        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n        self.fc_cell   = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, x):\n        \"\"\"\n        x: LongTensor (B, T_src)\n        returns:\n          enc_outputs: (B, T_src, hidden_size)  # concat of both directions\n          dec_init:    tuple (h0, c0), each (num_layers, B, hidden_size)\n        \"\"\"\n        B = x.size(0)\n        emb = self.dropout(self.embedding(x))             # (B,T,hidden_size)\n        outputs, (h, c) = self.rnn(emb)                   # h,c: (num_layers*2, B, h_per_dir)\n\n        # Take last layer's fwd/back states and fuse\n        # indices: last two slices are [-2] (forward), [-1] (backward)\n        h_last_f = h[-2]                                  # (B, h_per_dir)\n        h_last_b = h[-1]                                  # (B, h_per_dir)\n        c_last_f = c[-2]\n        c_last_b = c[-1]\n\n        h_cat = torch.cat([h_last_f, h_last_b], dim=1)    # (B, hidden_size)\n        c_cat = torch.cat([c_last_f, c_last_b], dim=1)    # (B, hidden_size)\n\n        # bridge to decoder size and stack for each layer\n        h0 = torch.tanh(self.fc_hidden(h_cat))            # (B, hidden_size)\n        c0 = torch.tanh(self.fc_cell(c_cat))              # (B, hidden_size)\n\n        # repeat to (num_layers, B, hidden_size)\n        h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        c0 = c0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n\n        # outputs already (B,T,hidden_size) because bidirectional concat\n        return outputs, (h0, c0)\n\n\n# ======================\n#  Luong-Attention Decoder (no extra methods; only __init__ and forward)\n# ======================\n\nclass DecoderRNN(nn.Module):\n    \"\"\"\n    Unidirectional LSTM decoder with Luong 'general' attention.\n    - hidden_size: decoder hidden/model size (must match encoder hidden_size)\n    - output_size: target vocab size\n    \"\"\"\n    def __init__(self, hidden_size=512, output_size=None, num_layers=2,\n                 emb_dropout=0.3, rnn_dropout=0.3, padding_idx=0):\n        super().__init__()\n        assert output_size is not None, \"output_size must be provided for decoder.\"\n\n        self.hidden_size = hidden_size\n        self.num_layers  = num_layers\n        self.output_size = output_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=padding_idx)\n        self.dropout   = nn.Dropout(emb_dropout)\n\n        # Luong 'general' attention: score(h_t, h_s) = h_t^T * W * h_s\n        self.attn_proj_enc = nn.Linear(hidden_size, hidden_size, bias=False)\n\n        # Decoder LSTM takes [emb_t ; context_t] as input each step\n        self.rnn = nn.LSTM(\n            input_size  = hidden_size + hidden_size,\n            hidden_size = hidden_size,\n            num_layers  = num_layers,\n            batch_first = True,\n            dropout = rnn_dropout if num_layers > 1 else 0.0,\n        )\n        self.fc_out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden, encoder_outputs):\n        \"\"\"\n        x:              (B,) or (B,1) or (1,B)  previous token ids\n        hidden:         (h, c), each (num_layers, B, hidden_size)\n        encoder_outputs:(B, T_src, hidden_size)\n        returns:\n          logits: (B,V)  # single step output\n          next_hidden: (h,c)\n        \"\"\"\n        # normalize shape\n        if x.dim() == 1:             # (B,)\n            x = x.unsqueeze(1)       # -> (B,1)\n        elif x.dim() == 2 and x.size(0) == 1:\n            x = x.transpose(0, 1)    # (1,B) -> (B,1)\n\n        B = x.size(0)\n        h, c = hidden\n        h_top = h[-1]                                # (B, hidden_size) last layer hidden\n\n        # Attention: energy = (W_enc * H_enc) @ h_top\n        enc_proj = self.attn_proj_enc(encoder_outputs)        # (B,T,hidden)\n        attn_scores = torch.bmm(enc_proj, h_top.unsqueeze(2)) # (B,T,1)\n        attn_weights = torch.softmax(attn_scores.squeeze(2), dim=1)  # (B,T)\n\n        # Context: sum_t (alpha_t * H_enc_t)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)  # (B,hidden)\n\n        # Prepare input to LSTM: concat(emb_t, context)\n        emb = self.dropout(self.embedding(x).squeeze(1))       # (B,hidden)\n        rnn_in = torch.cat([emb, context], dim=1).unsqueeze(1) # (B,1,hidden*2)\n\n        out, (h_next, c_next) = self.rnn(rnn_in, (h, c))       # out: (B,1,hidden)\n        logits = self.fc_out(out.squeeze(1))                   # (B,V)\n\n        return logits, (h_next, c_next)\n\n\n# ==============\n#  Seq2Seq wrapper (internal)\n# ==============\n\nclass Seq2Seq(nn.Module):\n    \"\"\"Internal wrapper so your external pipeline stays the same.\"\"\"\n    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self._enc_cache = None  # (enc_outputs, (h0,c0))\n\n    def encode(self, src):\n        self._enc_cache = self.encoder(src)   # (enc_outputs, (h0,c0))\n        return self._enc_cache\n\n    def decode_step(self, prev_tokens, hidden):\n        enc_outputs, _ = self._enc_cache\n        # pass encoder outputs automatically (no pipeline change required)\n        logits, next_hidden = self.decoder(prev_tokens, hidden, enc_outputs)\n        return logits, next_hidden\n\n\n# ======================\n#  Training / Inference (unchanged signatures)\n# ======================\n\ndef train_model(encoder, decoder, train_dl, tokenizer, epochs=20, lr=3e-4, teacher_forcing_ratio=0.7):\n    \"\"\"\n    Unchanged signature. Uses Seq2Seq internally.\n    \"\"\"\n    print(\"training started : \")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"device:\", device)\n\n    model = Seq2Seq(encoder.to(device), decoder.to(device)).to(device)\n\n    PAD = tokenizer.token_to_id[\"<PAD>\"]\n    criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n\n    enc_opt = optim.AdamW(model.encoder.parameters(), lr=lr, weight_decay=1e-4)\n    dec_opt = optim.AdamW(model.decoder.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, mode='min', patience=2, factor=0.5)\n\n    for epoch in range(epochs):\n        model.train()\n        total = 0.0\n\n        for src, tgt in train_dl:\n            src, tgt = src.to(device), tgt.to(device)\n            B = src.size(0)\n\n            enc_opt.zero_grad(set_to_none=True)\n            dec_opt.zero_grad(set_to_none=True)\n\n            # ---- encode ----\n            enc_outputs, dec_hidden = model.encode(src)  # (B,T,H), (h0,c0)\n\n            # ---- decode (teacher forcing) ----\n            dec_inp = tgt[:, 0]                          # (B,) <SOS>\n            logits_steps = []\n            for t in range(1, tgt.size(1)):\n                step_logits, dec_hidden = model.decode_step(dec_inp, dec_hidden)  # (B,V)\n                logits_steps.append(step_logits.unsqueeze(1))\n                use_tf = (np.random.rand() < teacher_forcing_ratio)\n                dec_inp = tgt[:, t] if use_tf else step_logits.argmax(-1)\n\n            logits = torch.cat(logits_steps, dim=1)                # (B, T-1, V)\n            loss = criterion(logits.reshape(-1, logits.size(-1)),  # (B*(T-1), V)\n                             tgt[:, 1:].reshape(-1))               # (B*(T-1),)\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.encoder.parameters(), 1.0)\n            torch.nn.utils.clip_grad_norm_(model.decoder.parameters(), 1.0)\n            enc_opt.step(); dec_opt.step()\n\n            total += float(loss.item())\n\n        avg = total / max(1, len(train_dl))\n        scheduler.step(avg)\n        print(f\"Epoch {epoch+1}/{epochs} | loss: {avg:.4f} | lr: {enc_opt.param_groups[0]['lr']:.6f}\")\n\n\n@torch.no_grad()\ndef translate(encoder, decoder, dataloader, tokenizer, seq_length=40):\n    \"\"\"\n    Unchanged signature. Uses Seq2Seq internally.\n    \"\"\"\n    device = next(encoder.parameters()).device\n    model = Seq2Seq(encoder.to(device), decoder.to(device)).eval()\n\n    SOS = tokenizer.token_to_id[\"<SOS>\"]\n    EOS = tokenizer.token_to_id[\"<EOS>\"]\n    PAD = tokenizer.token_to_id[\"<PAD>\"]\n\n    preds = []\n    for src, _ in dataloader:\n        src = src.to(device)\n        enc_outputs, dec_hidden = model.encode(src)\n\n        B = src.size(0)\n        dec_inp = torch.full((B,), SOS, dtype=torch.long, device=device)\n\n        seq_ids = []\n        for _ in range(seq_length):\n            step_logits, dec_hidden = model.decode_step(dec_inp, dec_hidden)  # (B,V)\n            next_ids = step_logits.argmax(-1)\n            seq_ids.append(next_ids.unsqueeze(1))\n            dec_inp = next_ids\n\n        seq_ids = torch.cat(seq_ids, dim=1)  # (B, seq_length)\n        for s in seq_ids:\n            ids = [i.item() for i in s if i.item() not in (SOS, EOS, PAD)]\n            preds.append(tokenizer.decode(ids))\n    return preds\n\nHIDDEN_SIZE = 1024  # also embedding dim\nEPOCHS = 70\nLR = 0.0001\nTF_RATIO = 0.78\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Bengali\nencoder_input_size_ben = int(max(np.max(en_train_encoded_ben), np.max(en_val_encoded_ben))) + 1\ndecoder_output_size_ben = int(max(np.max(de_train_encoded_ben), np.max(de_val_encoded_ben))) + 1\n\nencoder_ben = EncoderRNN(encoder_input_size_ben, hidden_size=HIDDEN_SIZE).to(device)\ndecoder_ben = DecoderRNN(hidden_size=HIDDEN_SIZE, output_size=decoder_output_size_ben).to(device)\n\nprint(\"Training Bengali model...\")\ntrain_model(encoder_ben, decoder_ben, train_dl_ben, ben_bpe,\n            epochs=EPOCHS, lr=LR, teacher_forcing_ratio=TF_RATIO)\n\n# Hindi\nencoder_input_size_hin = int(max(np.max(en_train_encoded_hin), np.max(en_val_encoded_hin))) + 1\ndecoder_output_size_hin = int(max(np.max(de_train_encoded_hin), np.max(de_val_encoded_hin))) + 1\n\nencoder_hin = EncoderRNN(encoder_input_size_hin, hidden_size=HIDDEN_SIZE).to(device)\ndecoder_hin = DecoderRNN(hidden_size=HIDDEN_SIZE, output_size=decoder_output_size_hin).to(device)\n\nprint(\"\\nTraining Hindi model...\")\ntrain_model(encoder_hin, decoder_hin, train_dl_hin, hin_bpe,\n            epochs=EPOCHS, lr=LR, teacher_forcing_ratio=TF_RATIO)\n\nprint(\"\\nGenerating Bengali translations...\")\nval_outs_ben = translate(encoder_ben, decoder_ben, test_dl_ben, ben_bpe)\ndf_ben = pd.DataFrame({\"ID\": val_ids_ben, \"Translation\": val_outs_ben})\ndf_ben.to_csv(\"answer_ben.csv\", index=False)\nprint(\"Saved Bengali predictions → answer_ben.csv\")\n\nprint(\"\\nGenerating Hindi translations\")\nval_outs_hin = translate(encoder_hin, decoder_hin, test_dl_hin, hin_bpe)\ndf_hin = pd.DataFrame({\"ID\": val_ids_hin, \"Translation\": val_outs_hin})\ndf_hin.to_csv(\"answer_hi.csv\", index=False)\nprint(\"Saved Hindi predictions → answer_hi.csv\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wV8IEUu2YHMa","outputId":"f73f868a-166c-4d62-8bcb-f97e50e43bcd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"210TJunwCkIC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nCreating final submission file\")\nimport pandas as pd\ndf_ben = pd.read_csv(\"/kaggle/input/erfiof\")\ndf_hin = pd.read_csv(\"/kaggle/input/fbthth\")\n\n# Combine Bengali first, then Hindi\ncombined_data = pd.concat([df_ben, df_hin], axis=0, ignore_index=True)\n\n# Save in the exact format required\nwith open(\"/kaggle/working/answer.csv\", \"w\") as f:\n    f.write(\"ID\\tTranslation\\n\")\n    for i in range(combined_data.shape[0]):\n        f.write(f\"{combined_data['ID'][i]}\\t{combined_data['Translation'][i]}\\n\")\n\nprint(\"Final submission file created → answer.csv\")\nprint(f\"Total rows: {combined_data.shape[0] + 1} (including header)\")  # +1 for header\nprint(f\"Bengali entries: {len(df_ben)}\")\nprint(f\"Hindi entries: {len(df_hin)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_1CiPwAFxvE","outputId":"1a930fe7-5475-4863-bc88-9ca2b2c205a8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"7hL3Ot2nvG6D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"hhu599TniH57","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"YMomXuZ9aT3r","trusted":true},"outputs":[],"execution_count":null}]}