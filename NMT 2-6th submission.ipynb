{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13431808,"sourceType":"datasetVersion","datasetId":8525257},{"sourceId":13433980,"sourceType":"datasetVersion","datasetId":8526762},{"sourceId":13434010,"sourceType":"datasetVersion","datasetId":8526784},{"sourceId":13480681,"sourceType":"datasetVersion","datasetId":8558629},{"sourceId":13480686,"sourceType":"datasetVersion","datasetId":8558633}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport json\nimport numpy as np\nimport re\nimport pandas as pd\nimport heapq\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom google.colab import drive","metadata":{"id":"lhLKQGTnuK8y","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:41.570065Z","iopub.execute_input":"2025-10-20T07:52:41.570662Z","iopub.status.idle":"2025-10-20T07:52:41.596857Z","shell.execute_reply.started":"2025-10-20T07:52:41.570640Z","shell.execute_reply":"2025-10-20T07:52:41.596228Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nwith open('/kaggle/input/new-data/train_data1.json', 'r') as file:\n    data_train = json.load(file)\n\nwith open('/kaggle/input/new-data/val_data1.json', 'r') as file:\n    data_val = json.load(file)","metadata":{"id":"_Wq9LVApuS_h","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:41.598038Z","iopub.execute_input":"2025-10-20T07:52:41.598279Z","iopub.status.idle":"2025-10-20T07:52:42.939341Z","shell.execute_reply.started":"2025-10-20T07:52:41.598261Z","shell.execute_reply":"2025-10-20T07:52:42.938554Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def extract_language_data_train(data, language_pair):\n    \"\"\"\n    Extracts train and validation sentences and IDs for a given language pair.\n\n    Args:\n        data (dict): Nested dictionary containing all language pairs.\n        language_pair (str): The language pair to extract (\"English-Bengali\", \"English-Hindi\", etc.)\n\n    Returns:\n        tuple: (source_train, target_train, train_ids)\n    \"\"\"\n    source_lst, target_lst, ids_lst = [], [], []\n\n    for lp, lp_data in data.items():\n        if lp == language_pair:\n            for data_type, data_entries in lp_data.items():\n                for entry_id, entry_data in data_entries.items():\n                    source = entry_data[\"source\"]\n                    target = entry_data[\"target\"]\n\n                    source_lst.append(source)\n                    target_lst.append(target)\n                    ids_lst.append(entry_id)\n\n    return source_lst, target_lst, ids_lst\n\n","metadata":{"id":"sVMigPDfuWlk","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:42.940193Z","iopub.execute_input":"2025-10-20T07:52:42.940400Z","iopub.status.idle":"2025-10-20T07:52:42.945534Z","shell.execute_reply.started":"2025-10-20T07:52:42.940383Z","shell.execute_reply":"2025-10-20T07:52:42.944811Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract_language_data_val(data , language_pair):\n\n  source_lst, ids_lst = [], []\n\n  for lp, lp_data in data.items():\n      if lp == language_pair:\n          for data_type, data_entries in lp_data.items():\n              for entry_id, entry_data in data_entries.items():\n                  source = entry_data[\"source\"]\n\n\n                  source_lst.append(source)\n\n                  ids_lst.append(entry_id)\n\n  return source_lst, ids_lst\n\n\n","metadata":{"id":"Ftil_iea6vQb","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:42.947266Z","iopub.execute_input":"2025-10-20T07:52:42.947441Z","iopub.status.idle":"2025-10-20T07:52:42.961305Z","shell.execute_reply.started":"2025-10-20T07:52:42.947426Z","shell.execute_reply":"2025-10-20T07:52:42.960691Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"source_train_ben, target_train_ben , train_ids_ben  = extract_language_data_train(data_train, \"English-Bengali\")\nsource_val_ben, val_ids_ben = extract_language_data_val(data_val, \"English-Bengali\")\n\nsource_train_hin, target_train_hin, train_ids_hin = extract_language_data_train(data_train, \"English-Hindi\")\nsource_val_hin, val_ids_hin = extract_language_data_val(data_val, \"English-Hindi\")\n","metadata":{"id":"4DpKujpk3cu2","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:42.961999Z","iopub.execute_input":"2025-10-20T07:52:42.962233Z","iopub.status.idle":"2025-10-20T07:52:43.014005Z","shell.execute_reply.started":"2025-10-20T07:52:42.962211Z","shell.execute_reply":"2025-10-20T07:52:43.013414Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Node:\n    def __init__(self, token_id):\n        self.token_id = token_id\n        self.prev = None\n        self.next = None\n\nclass BPETokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.inverse_vocab = {}\n        self.merges = {}\n        self.token_to_id = {}\n        self.id_to_token = {}\n\n    def initialize_vocab(self):\n        reserved = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n        self.vocab = {i: tok for i, tok in enumerate(reserved)}\n        self.inverse_vocab = {tok: i for i, tok in self.vocab.items()}\n        return len(self.vocab)\n\n    def build_corpus(self, text, next_token_id):\n        words = [list(w) + [\"</w>\"] for w in text.strip().split()]\n        corpus = []\n        for w in words:\n            head, prev = None, None\n            for ch in w:\n                if ch not in self.inverse_vocab:\n                    self.vocab[next_token_id] = ch\n                    self.inverse_vocab[ch] = next_token_id\n                    next_token_id += 1\n                node = Node(self.inverse_vocab[ch])\n                if prev:\n                    prev.next, node.prev = node, prev\n                else:\n                    head = node\n                prev = node\n            corpus.append(head)\n        return corpus, next_token_id\n\n    def count_pairs(self, corpus):\n        pair_occurrences = defaultdict(set)\n        for head in corpus:\n            node = head\n            while node and node.next:\n                pair_occurrences[(node.token_id, node.next.token_id)].add(node)\n                node = node.next\n        return pair_occurrences\n\n    def merge_pair(self, pair, new_id, pair_occurrences, heap):\n        t1, t2 = pair\n        new_tok = self.vocab[t1] + self.vocab[t2]\n        self.vocab[new_id] = new_tok\n        self.inverse_vocab[new_tok] = new_id\n        affected = list(pair_occurrences[pair])\n        pair_occurrences[pair].clear()\n        for node in affected:\n            if not node.next or node.token_id != t1 or node.next.token_id != t2:\n                continue\n            node.token_id = new_id\n            removed = node.next\n            node.next = removed.next\n            if removed.next:\n                removed.next.prev = node\n            if node.prev:\n                old = (node.prev.token_id, t1)\n                pair_occurrences[old].discard(node.prev)\n                new = (node.prev.token_id, node.token_id)\n                pair_occurrences[new].add(node.prev)\n                heapq.heappush(heap, (-len(pair_occurrences[new]), new))\n            if node.next:\n                old = (t2, node.next.token_id)\n                pair_occurrences[old].discard(node)\n                new = (node.token_id, node.next.token_id)\n                pair_occurrences[new].add(node)\n                heapq.heappush(heap, (-len(pair_occurrences[new]), new))\n\n    def train(self, text, vocab_size=5000):\n        next_id = self.initialize_vocab()\n        corpus, next_id = self.build_corpus(text, next_id)\n        pair_occurrences = self.count_pairs(corpus)\n        heap = [(-len(nodes), pair) for pair, nodes in pair_occurrences.items()]\n        heapq.heapify(heap)\n        while len(self.vocab) < vocab_size and heap:\n            freq, pair = heapq.heappop(heap)\n            freq = -freq\n            if freq == 0 or len(pair_occurrences[pair]) != freq:\n                continue\n            self.merges[pair] = next_id\n            self.merge_pair(pair, next_id, pair_occurrences, heap)\n            next_id += 1\n        self.token_to_id = {tok: tid for tid, tok in self.vocab.items()}\n        self.id_to_token = {tid: tok for tid, tok in self.vocab.items()}\n\n    def tokenize(self, text):\n        words = [list(w) + [\"</w>\"] for w in text.strip().split()]\n        tokens = []\n        for w in words:\n            ids = [self.inverse_vocab.get(ch, self.token_to_id[\"<UNK>\"]) for ch in w]\n            merged = True\n            while merged:\n                merged, i = False, 0\n                while i < len(ids) - 1:\n                    pair = (ids[i], ids[i + 1])\n                    if pair in self.merges:\n                        ids[i] = self.merges[pair]\n                        ids.pop(i + 1)\n                        merged = True\n                    else:\n                        i += 1\n            tokens.extend(ids)\n        return tokens\n\n\n    def decode(self, token_ids):\n        specials = {self.token_to_id.get(\"<PAD>\"), self.token_to_id.get(\"<UNK>\"),\n                    self.token_to_id.get(\"<SOS>\"), self.token_to_id.get(\"<EOS>\")}\n        words, cur = [], []\n        for tid in token_ids:\n            if tid in specials:\n                continue\n            tok = self.id_to_token.get(tid, \"<UNK>\")\n            if tok.endswith(\"</w>\"):\n                cur.append(tok[:-4])\n                if cur:\n                    words.append(\"\".join(cur))\n                cur = []\n            else:\n                cur.append(tok)\n        if cur:\n            words.append(\"\".join(cur))\n        return \" \".join(words).strip()\n","metadata":{"id":"1uC8f9WpuZqy","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:43.014714Z","iopub.execute_input":"2025-10-20T07:52:43.014960Z","iopub.status.idle":"2025-10-20T07:52:43.056835Z","shell.execute_reply.started":"2025-10-20T07:52:43.014938Z","shell.execute_reply":"2025-10-20T07:52:43.056084Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocess_text(text, lang=\"en\"):\n    \"\"\"Cleans and normalizes text before tokenization.\"\"\"\n    text = text.strip()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[“”‘’]', '\"', text)\n    if lang == \"hi\":\n        text = re.sub(r'[^\\u0900-\\u097F\\s.,!?]', '', text)\n    elif lang == \"bn\":\n        text = re.sub(r'[^\\u0980-\\u09FF\\s.,!?।]', '', text)\n    else:\n        text = re.sub(r'[^\\w\\s.,!?]', '', text)\n        text = text.lower()\n    return text","metadata":{"id":"HPghiLl3Abkv","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:43.057502Z","iopub.execute_input":"2025-10-20T07:52:43.057735Z","iopub.status.idle":"2025-10-20T07:52:43.072022Z","shell.execute_reply.started":"2025-10-20T07:52:43.057716Z","shell.execute_reply":"2025-10-20T07:52:43.071418Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"eng_ben_bpe = BPETokenizer()\neng_ben_bpe.train(\" \".join([preprocess_text(s, \"en\") for s in source_train_ben]), vocab_size=20000)\n\nben_bpe = BPETokenizer()\nben_bpe.train(\" \".join([preprocess_text(s, \"bn\") for s in target_train_ben]), vocab_size=20000)","metadata":{"id":"CH25hMMEApEq","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:52:43.072783Z","iopub.execute_input":"2025-10-20T07:52:43.073242Z","iopub.status.idle":"2025-10-20T07:54:23.566283Z","shell.execute_reply.started":"2025-10-20T07:52:43.073224Z","shell.execute_reply":"2025-10-20T07:54:23.565724Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"bengali_text = \"এই জায়গাগুলো দেখতে ভুলো না।\"\ntokens = ben_bpe.tokenize(bengali_text)\ndecoded_sentence = ben_bpe.decode(tokens)\nprint(decoded_sentence)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zihyhs3d75CY","outputId":"c13ad82a-7016-487c-ebb4-85a34b152eb1","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:54:23.566977Z","iopub.execute_input":"2025-10-20T07:54:23.567317Z","iopub.status.idle":"2025-10-20T07:54:23.572088Z","shell.execute_reply.started":"2025-10-20T07:54:23.567294Z","shell.execute_reply":"2025-10-20T07:54:23.571490Z"}},"outputs":[{"name":"stdout","text":"এই জায়গাগুলো দেখতে ভুলো না।\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"eng_hin_bpe = BPETokenizer()\neng_hin_bpe.train(\" \".join([preprocess_text(s, \"en\") for s in source_train_hin]), vocab_size=20000)\n\nhin_bpe = BPETokenizer()\nhin_bpe.train(\" \".join([preprocess_text(s, \"hi\") for s in target_train_hin]), vocab_size=20000)","metadata":{"id":"0TMnO5CrA3IR","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:54:23.574072Z","iopub.execute_input":"2025-10-20T07:54:23.574506Z","iopub.status.idle":"2025-10-20T07:56:27.515222Z","shell.execute_reply.started":"2025-10-20T07:54:23.574488Z","shell.execute_reply":"2025-10-20T07:56:27.514624Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"hindi_text = \"इन जगहों को देखना मत भूलना।\"\ntokens = hin_bpe.tokenize(hindi_text)\nprint(\"Tokens:\", tokens)\ndecoded_sentence = hin_bpe.decode(tokens)\nprint(\"Decoded Sentence:\", decoded_sentence)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsweF7c387P6","outputId":"d94d34b5-8619-452b-db83-b89a89f77e4c","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:56:27.515909Z","iopub.execute_input":"2025-10-20T07:56:27.516088Z","iopub.status.idle":"2025-10-20T07:56:27.520906Z","shell.execute_reply.started":"2025-10-20T07:56:27.516074Z","shell.execute_reply":"2025-10-20T07:56:27.520279Z"}},"outputs":[{"name":"stdout","text":"Tokens: [431, 9200, 137, 4572, 1906, 35, 46, 10, 3888]\nDecoded Sentence: इन जगहों को देखना मत भूलना।\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for tokenizer in [ben_bpe, hin_bpe]:\n    for t in [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n        if t not in tokenizer.token_to_id:\n            new_id = len(tokenizer.token_to_id)\n            tokenizer.token_to_id[t] = new_id\n            tokenizer.id_to_token[new_id] = t","metadata":{"id":"aTvO-lf7Y_iV","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:56:27.521503Z","iopub.execute_input":"2025-10-20T07:56:27.521759Z","iopub.status.idle":"2025-10-20T07:56:27.534371Z","shell.execute_reply.started":"2025-10-20T07:56:27.521738Z","shell.execute_reply":"2025-10-20T07:56:27.533564Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"seq_length = 30\n\ndef encode_and_pad(tokenizer, sent, max_length):\n    sos = [tokenizer.token_to_id[\"<SOS>\"]]\n    eos = [tokenizer.token_to_id[\"<EOS>\"]]\n    pad = [tokenizer.token_to_id[\"<PAD>\"]]\n    encoded = tokenizer.tokenize(sent)\n    if len(encoded) < max_length - 2:\n        n_pads = max_length - 2 - len(encoded)\n        return sos + encoded + eos + pad * n_pads\n    else:\n        return sos + encoded[:max_length - 2] + eos","metadata":{"id":"_gwPj_JsuwMA","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:56:27.535070Z","iopub.execute_input":"2025-10-20T07:56:27.535265Z","iopub.status.idle":"2025-10-20T07:56:27.542907Z","shell.execute_reply.started":"2025-10-20T07:56:27.535251Z","shell.execute_reply":"2025-10-20T07:56:27.542278Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"en_train_encoded_ben = [encode_and_pad(eng_ben_bpe, sent, seq_length) for sent in source_train_ben]\nde_train_encoded_ben = [encode_and_pad(ben_bpe, sent, seq_length) for sent in target_train_ben]\nen_val_encoded_ben = [encode_and_pad(eng_ben_bpe, sent, seq_length) for sent in source_val_ben]\nde_val_encoded_ben = [encode_and_pad(ben_bpe, sent, seq_length) for sent in source_val_ben]\n\n\nen_train_encoded_hin = [encode_and_pad(eng_hin_bpe, sent, seq_length) for sent in source_train_hin]\nde_train_encoded_hin = [encode_and_pad(hin_bpe, sent, seq_length) for sent in target_train_hin]\nen_val_encoded_hin = [encode_and_pad(eng_hin_bpe, sent, seq_length) for sent in source_val_hin]\nde_val_encoded_hin = [encode_and_pad(hin_bpe, sent, seq_length) for sent in source_val_hin]\n\n\n\ntrain_x_ben = np.array(en_train_encoded_ben)\ntrain_y_ben = np.array(de_train_encoded_ben)\ntest_x_ben = np.array(en_val_encoded_ben)\ntest_y_ben = np.array(de_val_encoded_ben)\n\ntrain_x_hin = np.array(en_train_encoded_hin)\ntrain_y_hin = np.array(de_train_encoded_hin)\ntest_x_hin = np.array(en_val_encoded_hin)\ntest_y_hin = np.array(de_val_encoded_hin)\n\nbatch_size =100\n\ntrain_ds_ben = TensorDataset(torch.from_numpy(train_x_ben), torch.from_numpy(train_y_ben))\ntest_ds_ben = TensorDataset(torch.from_numpy(test_x_ben), torch.from_numpy(test_y_ben))\ntrain_dl_ben = DataLoader(train_ds_ben, shuffle=True, batch_size=batch_size, drop_last=True)\ntest_dl_ben = DataLoader(test_ds_ben, shuffle=False, batch_size=batch_size)\n\n\ntrain_ds_hin = TensorDataset(torch.from_numpy(train_x_hin), torch.from_numpy(train_y_hin))\ntest_ds_hin = TensorDataset(torch.from_numpy(test_x_hin), torch.from_numpy(test_y_hin))\ntrain_dl_hin = DataLoader(train_ds_hin, shuffle=True, batch_size=batch_size, drop_last=True)\ntest_dl_hin = DataLoader(test_ds_hin, shuffle=False, batch_size=batch_size)","metadata":{"id":"eL7W8--JuhWR","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:56:27.543657Z","iopub.execute_input":"2025-10-20T07:56:27.543879Z","iopub.status.idle":"2025-10-20T07:56:48.320478Z","shell.execute_reply.started":"2025-10-20T07:56:27.543846Z","shell.execute_reply":"2025-10-20T07:56:48.319889Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import numpy as np\n# import pandas as pd\n\n# class EncoderRNN(nn.Module):\n#     def __init__(self, input_size, hidden_size):\n#         super(EncoderRNN, self).__init__()\n#         self.hidden_size = hidden_size\n#         self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=0)\n#         self.dropout = nn.Dropout(0.3)  # CHANGE: Added dropout for regularization\n#         self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2, batch_first=True, dropout=0.3)  # CHANGE: 2-layer GRU\n\n#     def forward(self, x, hidden):\n#         embedded = self.dropout(self.embedding(x))\n#         output, hidden = self.gru(embedded, hidden)\n#         return output, hidden\n\n#     def init_hidden(self, batch_size, device):\n#         return torch.zeros(2, batch_size, self.hidden_size, device=device)  # CHANGE: num_layers=2\n\n\n# class DecoderRNN(nn.Module):\n#     def __init__(self, hidden_size, output_size):\n#         super(DecoderRNN, self).__init__()\n#         self.hidden_size = hidden_size\n#         self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n#         self.dropout = nn.Dropout(0.3)  \n#         self.gru = nn.GRU(hidden_size, hidden_size, num_layers=2, batch_first=True, dropout=0.3)  # CHANGE\n#         self.fc = nn.Linear(hidden_size, output_size)\n\n#     def forward(self, x, hidden):\n#         embedded = self.dropout(self.embedding(x))\n#         output, hidden = self.gru(embedded, hidden)\n#         output = self.fc(output)\n#         return output, hidden\n\n\n\n# def train_model(encoder, decoder, train_dl, tokenizer, epochs, lr=0.0025, teacher_forcing_ratio=0.8):  #CHANGE: lower lr\n#     print(\"training started : \")\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(device)\n#     encoder.to(device)\n#     decoder.to(device)\n#     criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id[\"<PAD>\"])\n#     enc_opt = optim.AdamW(encoder.parameters(), lr=lr, weight_decay=1e-4)  #CHANGE: AdamW optimizer\n#     dec_opt = optim.AdamW(decoder.parameters(), lr=lr, weight_decay=1e-4)\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, 'min', patience=2, factor=0.5)  #CHANGE: LR scheduler\n#     # criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id[\"<PAD>\"])\n#     # enc_opt = optim.Adagrad(encoder.parameters(), lr=lr, weight_decay=1e-5)\n#     # dec_opt = optim.Adagrad(decoder.parameters(), lr=lr, weight_decay=1e-5)\n#     # scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, 'min', patience=3, factor=0.5)\n\n\n#     for epoch in range(epochs):\n#         encoder.train()\n#         decoder.train()\n#         total_loss = 0\n\n#         for src, tgt in train_dl:\n#             batch_size = src.size(0)\n#             src, tgt = src.to(device), tgt.to(device)\n#             enc_hidden = encoder.init_hidden(batch_size, device)\n\n#             enc_opt.zero_grad()\n#             dec_opt.zero_grad()\n\n#             # Encode\n#             _, hidden = encoder(src, enc_hidden)\n\n#             # Initialize decoder input with <SOS>\n#             dec_input = tgt[:, 0].unsqueeze(1)\n#             dec_hidden = hidden\n#             outputs = []\n\n#             # Step-by-step decoding\n#             for t in range(1, tgt.size(1)):\n#                 out, dec_hidden = decoder(dec_input, dec_hidden)\n#                 pred = out[:, -1, :]\n#                 outputs.append(pred.unsqueeze(1))\n\n#                 teacher_force = np.random.rand() < teacher_forcing_ratio\n#                 next_input = tgt[:, t] if teacher_force else pred.argmax(1)\n#                 dec_input = next_input.unsqueeze(1)\n\n#             outputs = torch.cat(outputs, dim=1)\n#             loss = criterion(outputs.reshape(-1, outputs.shape[-1]), tgt[:, 1:].reshape(-1))\n#             loss.backward()\n\n#             # CHANGE: Gradient clipping for stability\n#             torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n#             torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n#             enc_opt.step()\n#             dec_opt.step()\n\n#             total_loss += loss.item()\n\n#         avg_loss = total_loss / len(train_dl)\n#         scheduler.step(avg_loss)  # CHANGE: adjust learning rate dynamically\n#         print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, LR: {enc_opt.param_groups[0]['lr']:.6f}\")\n\n\n# def translate(encoder, decoder, dataloader, tokenizer, seq_length=40):\n#     encoder.eval()\n#     decoder.eval()\n#     device = next(encoder.parameters()).device\n#     SOS = tokenizer.token_to_id[\"<SOS>\"]\n#     EOS = tokenizer.token_to_id[\"<EOS>\"]\n#     preds = []\n\n#     with torch.no_grad():\n#         for src, _ in dataloader:\n#             batch_size = src.size(0)\n#             src = src.to(device)\n#             enc_hidden = encoder.init_hidden(batch_size, device)\n#             _, hidden = encoder(src, enc_hidden)\n\n#             dec_input = torch.full((batch_size, 1), SOS, dtype=torch.long, device=device)\n#             dec_hidden = hidden\n#             seq_preds = []\n\n#             for _ in range(seq_length):\n#                 dec_out, dec_hidden = decoder(dec_input, dec_hidden)\n#                 dec_out = dec_out.argmax(-1)\n#                 seq_preds.append(dec_out)\n#                 dec_input = dec_out\n\n#             seq_preds = torch.stack(seq_preds, dim=1)\n#             for s in seq_preds:\n#                ids = [i.item() for i in s if i.item() not in (SOS, EOS, tokenizer.token_to_id[\"<PAD>\"])]\n#                preds.append(tokenizer.decode(ids))\n#     return preds\n\n\n# hidden_size = 256\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# encoder_input_size_ben = max(np.max(en_train_encoded_ben), np.max(en_val_encoded_ben)) + 1\n# decoder_output_size_ben = max(np.max(de_train_encoded_ben), np.max(de_val_encoded_ben)) + 1\n\n# encoder_ben = EncoderRNN(encoder_input_size_ben, hidden_size).to(device)\n# decoder_ben = DecoderRNN(hidden_size, decoder_output_size_ben).to(device)\n\n# print(\"Training Bengali model...\")\n# train_model(encoder_ben, decoder_ben, train_dl_ben, ben_bpe, epochs=5, lr=0.001, teacher_forcing_ratio=0.6)  #CHANGE: longer training, higher TF ratio\n\n\n\n# encoder_input_size_hin = max(np.max(en_train_encoded_hin), np.max(en_val_encoded_hin)) + 1\n# decoder_output_size_hin = max(np.max(de_train_encoded_hin), np.max(de_val_encoded_hin)) + 1\n\n# encoder_hin = EncoderRNN(encoder_input_size_hin, hidden_size).to(device)\n# decoder_hin = DecoderRNN(hidden_size, decoder_output_size_hin).to(device)\n\n# print(\"\\nTraining Hindi model...\")\n# train_model(encoder_hin, decoder_hin, train_dl_hin, hin_bpe, epochs=5, lr=0.001, teacher_forcing_ratio=0.6)  #CHANGE\n\n\n\n# print(\"\\nGenerating Bengali translations...\")\n# val_outs_ben = translate(encoder_ben, decoder_ben, test_dl_ben, ben_bpe)\n# df_ben = pd.DataFrame({\"ID\": val_ids_ben, \"Translation\": val_outs_ben})\n# df_ben.to_csv(\"answer_ben.csv\", index=False)\n# print(\"Saved Bengali predictions → answer_ben.csv\")\n\n# print(\"\\nGenerating Hindi translations...\")\n# val_outs_hin = translate(encoder_hin, decoder_hin, test_dl_hin, hin_bpe)\n# df_hin = pd.DataFrame({\"ID\": val_ids_hin, \"Translation\": val_outs_hin})\n# df_hin.to_csv(\"answer_hi.csv\", index=False)\n# print(\"Saved Hindi predictions → answer_hi.csv\")\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\n\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size=512, num_layers=2, emb_dropout=0.2, rnn_dropout=0.2):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=0)\n        self.dropout = nn.Dropout(emb_dropout)\n        self.lstm = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=rnn_dropout if num_layers > 1 else 0.0\n        )\n\n    def forward(self, x, hidden):\n        \"\"\"\n        x: (B, T) long\n        hidden:  LSTM (h0, c0)\n        returns:\n          outputs: (B, T, H)\n          hidden: (hT, cT)\n        \"\"\"\n        B = x.size(0)\n        device = x.device\n\n        h0, c0 = self._to_lstm_hidden(hidden, B, device)\n        emb = self.dropout(self.embedding(x))  # (B,T,H)\n        outputs, (hT, cT) = self.lstm(emb, (h0, c0))\n        return outputs, (hT, cT)\n\n    def init_hidden(self, batch_size, device):\n        \"\"\"\n        Backward-compatible: returns GRU-style h0 (num_layers,B,H).\n        We create c0 internally in forward().\n        \"\"\"\n        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n\n    def _to_lstm_hidden(self, hidden, batch_size, device):\n        if isinstance(hidden, tuple):\n            h0, c0 = hidden\n        else:\n            h0 = hidden\n            c0 = torch.zeros_like(h0, device=device)\n        return h0, c0\n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, hidden_size=512, output_size=None, num_layers=2, emb_dropout=0.3, rnn_dropout=0.3):\n        super(DecoderRNN, self).__init__()\n        assert output_size is not None, \"output_size (target vocab size)\"\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.output_size = output_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n        self.dropout = nn.Dropout(emb_dropout)\n        self.lstm = nn.LSTM(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=rnn_dropout if num_layers > 1 else 0.0\n        )\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        \"\"\"\n        x: (B,T) or (B,) or (1,B)\n        hidden: LSTM (h,c)\n        returns:\n          logits: (B,T,V) or (B,V) for single-step\n          hidden: (hT, cT)\n        \"\"\"\n        x, squeeze_back = self._normalize_input_shape(x)\n        B = x.size(0)\n        device = x.device\n\n        h0, c0 = self._to_lstm_hidden(hidden, B, device)\n\n        emb = self.dropout(self.embedding(x))        # (B,T,H)\n        out, (hT, cT) = self.lstm(emb, (h0, c0))     # (B,T,H)\n        logits = self.fc(out)                        # (B,T,V)\n\n        if squeeze_back:\n            logits = logits.squeeze(1)               # (B,V)\n        return logits, (hT, cT)\n\n    def _normalize_input_shape(self, x):\n        squeeze_back = False\n        if x.dim() == 1:\n            x = x.unsqueeze(1)         # (B,) -> (B,1)\n            squeeze_back = True\n        elif x.dim() == 2 and x.size(0) == 1:\n            x = x.transpose(0, 1)      # (1,B) -> (B,1)\n            squeeze_back = True\n        return x, squeeze_back\n\n    def _to_lstm_hidden(self, hidden, batch_size, device):\n        if isinstance(hidden, tuple):\n            return hidden\n        else:\n            h = hidden\n            c = torch.zeros_like(h, device=device)\n            return (h, c)\n\n\ndef train_model(encoder, decoder, train_dl, tokenizer, epochs=600, lr=0.0001, teacher_forcing_ratio=0.78):\n    \"\"\"\n    train_dl yield: (src, tgt)\n      - src: (B, T_src) LongTensor\n      - tgt: (B, T_tgt) LongTensor with tgt[:,0] == <SOS>\n    tokenizer: used only to get PAD id for loss ignore\n    \"\"\"\n    print(\"training started : \")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # print(\"device:\", device)\n\n    encoder.to(device)\n    decoder.to(device)\n\n    pad_id = tokenizer.token_to_id[\"<PAD>\"]\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n\n    enc_opt = optim.AdamW(encoder.parameters(), lr=lr, weight_decay=1e-4)\n    dec_opt = optim.AdamW(decoder.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(enc_opt, mode='min', patience=2, factor=0.5)\n\n    for epoch in range(epochs):\n        encoder.train()\n        decoder.train()\n        total_loss = 0.0\n\n        for src, tgt in train_dl:\n            src, tgt = src.to(device), tgt.to(device)\n            batch_size = src.size(0)\n\n            # init encoder hidden (GRU-style h0; c0 is created internally)\n            enc_hidden = encoder.init_hidden(batch_size, device)\n\n            enc_opt.zero_grad(set_to_none=True)\n            dec_opt.zero_grad(set_to_none=True)\n\n            # Encode\n            _, enc_last = encoder(src, enc_hidden)    # enc_last is (hT, cT)\n\n            # Decode with teacher forcing\n            dec_hidden = enc_last\n            dec_input = tgt[:, 0]                     # (B,) <SOS>\n            outputs = []\n\n            for t in range(1, tgt.size(1)):\n                # Decoder step: returns logits (B,V) for single-step input\n                step_logits, dec_hidden = decoder(dec_input, dec_hidden)   # (B,V)\n                outputs.append(step_logits.unsqueeze(1))                   # (B,1,V)\n\n                teacher_force = (np.random.rand() < teacher_forcing_ratio)\n                next_input = tgt[:, t] if teacher_force else step_logits.argmax(-1)\n                dec_input = next_input\n\n            outputs = torch.cat(outputs, dim=1)       # (B, T_tgt-1, V)\n            loss = criterion(\n                outputs.reshape(-1, outputs.size(-1)),   # (B*(T-1), V)\n                tgt[:, 1:].reshape(-1)                   # (B*(T-1),)\n            )\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n            torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1.0)\n            enc_opt.step()\n            dec_opt.step()\n\n            total_loss += float(loss.item())\n\n        avg_loss = total_loss / max(1, len(train_dl))\n        scheduler.step(avg_loss)\n        print(f\"Epoch {epoch+1}/{epochs} | loss: {avg_loss:.4f} | lr: {enc_opt.param_groups[0]['lr']:.6f}\")\n\n\n@torch.no_grad()\ndef translate(encoder, decoder, dataloader, tokenizer, seq_length=40):\n    encoder.eval()\n    decoder.eval()\n    device = next(encoder.parameters()).device\n\n    SOS = tokenizer.token_to_id[\"<SOS>\"]\n    EOS = tokenizer.token_to_id[\"<EOS>\"]\n    PAD = tokenizer.token_to_id[\"<PAD>\"]\n\n    preds = []\n    for src, _ in dataloader:\n        src = src.to(device)\n        B = src.size(0)\n\n        enc_hidden = encoder.init_hidden(B, device)\n        _, enc_last = encoder(src, enc_hidden)\n\n        dec_hidden = enc_last\n        dec_input = torch.full((B,), SOS, dtype=torch.long, device=device)\n\n        seq_preds = []\n        for _ in range(seq_length):\n            step_logits, dec_hidden = decoder(dec_input, dec_hidden)  # (B,V)\n            next_ids = step_logits.argmax(-1)                         # (B,)\n            seq_preds.append(next_ids.unsqueeze(1))                   # (B,1)\n            dec_input = next_ids\n\n        seq_preds = torch.cat(seq_preds, dim=1)  # (B, seq_length)\n\n        # detokenize each sequence\n        for s in seq_preds:\n            ids = [i.item() for i in s if i.item() not in (SOS, EOS, PAD)]\n            preds.append(tokenizer.decode(ids))\n    return preds\n\n\nHIDDEN_SIZE = 1024  # also embedding dim\nEPOCHS = 70\nLR = 0.0001\nTF_RATIO = 0.78\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Bengali\nencoder_input_size_ben = int(max(np.max(en_train_encoded_ben), np.max(en_val_encoded_ben))) + 1\ndecoder_output_size_ben = int(max(np.max(de_train_encoded_ben), np.max(de_val_encoded_ben))) + 1\n\nencoder_ben = EncoderRNN(encoder_input_size_ben, hidden_size=HIDDEN_SIZE).to(device)\ndecoder_ben = DecoderRNN(hidden_size=HIDDEN_SIZE, output_size=decoder_output_size_ben).to(device)\n\nprint(\"Training Bengali model...\")\ntrain_model(encoder_ben, decoder_ben, train_dl_ben, ben_bpe,\n            epochs=EPOCHS, lr=LR, teacher_forcing_ratio=TF_RATIO)\n\n# Hindi\nencoder_input_size_hin = int(max(np.max(en_train_encoded_hin), np.max(en_val_encoded_hin))) + 1\ndecoder_output_size_hin = int(max(np.max(de_train_encoded_hin), np.max(de_val_encoded_hin))) + 1\n\nencoder_hin = EncoderRNN(encoder_input_size_hin, hidden_size=HIDDEN_SIZE).to(device)\ndecoder_hin = DecoderRNN(hidden_size=HIDDEN_SIZE, output_size=decoder_output_size_hin).to(device)\n\nprint(\"\\nTraining Hindi model...\")\ntrain_model(encoder_hin, decoder_hin, train_dl_hin, hin_bpe,\n            epochs=EPOCHS, lr=LR, teacher_forcing_ratio=TF_RATIO)\n\nprint(\"\\nGenerating Bengali translations...\")\nval_outs_ben = translate(encoder_ben, decoder_ben, test_dl_ben, ben_bpe)\ndf_ben = pd.DataFrame({\"ID\": val_ids_ben, \"Translation\": val_outs_ben})\ndf_ben.to_csv(\"answer_ben.csv\", index=False)\nprint(\"Saved Bengali predictions → answer_ben.csv\")\n\nprint(\"\\nGenerating Hindi translations\")\nval_outs_hin = translate(encoder_hin, decoder_hin, test_dl_hin, hin_bpe)\ndf_hin = pd.DataFrame({\"ID\": val_ids_hin, \"Translation\": val_outs_hin})\ndf_hin.to_csv(\"answer_hi.csv\", index=False)\nprint(\"Saved Hindi predictions → answer_hi.csv\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wV8IEUu2YHMa","outputId":"f73f868a-166c-4d62-8bcb-f97e50e43bcd","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T07:56:48.321378Z","iopub.execute_input":"2025-10-20T07:56:48.321648Z","iopub.status.idle":"2025-10-20T15:15:35.546247Z","shell.execute_reply.started":"2025-10-20T07:56:48.321627Z","shell.execute_reply":"2025-10-20T15:15:35.545496Z"}},"outputs":[{"name":"stdout","text":"Training Bengali model...\ntraining started : \ndevice: cuda\nEpoch 1/70 | loss: 5.9290 | lr: 0.000100\nEpoch 2/70 | loss: 5.2866 | lr: 0.000100\nEpoch 3/70 | loss: 5.0368 | lr: 0.000100\nEpoch 4/70 | loss: 4.8608 | lr: 0.000100\nEpoch 5/70 | loss: 4.7177 | lr: 0.000100\nEpoch 6/70 | loss: 4.6170 | lr: 0.000100\nEpoch 7/70 | loss: 4.5278 | lr: 0.000100\nEpoch 8/70 | loss: 4.4369 | lr: 0.000100\nEpoch 9/70 | loss: 4.3693 | lr: 0.000100\nEpoch 10/70 | loss: 4.2974 | lr: 0.000100\nEpoch 11/70 | loss: 4.2387 | lr: 0.000100\nEpoch 12/70 | loss: 4.1848 | lr: 0.000100\nEpoch 13/70 | loss: 4.1388 | lr: 0.000100\nEpoch 14/70 | loss: 4.0800 | lr: 0.000100\nEpoch 15/70 | loss: 4.0258 | lr: 0.000100\nEpoch 16/70 | loss: 3.9797 | lr: 0.000100\nEpoch 17/70 | loss: 3.9359 | lr: 0.000100\nEpoch 18/70 | loss: 3.8949 | lr: 0.000100\nEpoch 19/70 | loss: 3.8503 | lr: 0.000100\nEpoch 20/70 | loss: 3.7987 | lr: 0.000100\nEpoch 21/70 | loss: 3.7527 | lr: 0.000100\nEpoch 22/70 | loss: 3.7221 | lr: 0.000100\nEpoch 23/70 | loss: 3.6860 | lr: 0.000100\nEpoch 24/70 | loss: 3.6387 | lr: 0.000100\nEpoch 25/70 | loss: 3.6042 | lr: 0.000100\nEpoch 26/70 | loss: 3.5766 | lr: 0.000100\nEpoch 27/70 | loss: 3.5242 | lr: 0.000100\nEpoch 28/70 | loss: 3.4810 | lr: 0.000100\nEpoch 29/70 | loss: 3.4531 | lr: 0.000100\nEpoch 30/70 | loss: 3.3991 | lr: 0.000100\nEpoch 31/70 | loss: 3.3781 | lr: 0.000100\nEpoch 32/70 | loss: 3.3425 | lr: 0.000100\nEpoch 33/70 | loss: 3.2957 | lr: 0.000100\nEpoch 34/70 | loss: 3.2654 | lr: 0.000100\nEpoch 35/70 | loss: 3.2228 | lr: 0.000100\nEpoch 36/70 | loss: 3.1813 | lr: 0.000100\nEpoch 37/70 | loss: 3.1483 | lr: 0.000100\nEpoch 38/70 | loss: 3.1302 | lr: 0.000100\nEpoch 39/70 | loss: 3.0843 | lr: 0.000100\nEpoch 40/70 | loss: 3.0557 | lr: 0.000100\nEpoch 41/70 | loss: 3.0156 | lr: 0.000100\nEpoch 42/70 | loss: 2.9991 | lr: 0.000100\nEpoch 43/70 | loss: 2.9388 | lr: 0.000100\nEpoch 44/70 | loss: 2.9166 | lr: 0.000100\nEpoch 45/70 | loss: 2.8874 | lr: 0.000100\nEpoch 46/70 | loss: 2.8607 | lr: 0.000100\nEpoch 47/70 | loss: 2.8306 | lr: 0.000100\nEpoch 48/70 | loss: 2.7831 | lr: 0.000100\nEpoch 49/70 | loss: 2.7643 | lr: 0.000100\nEpoch 50/70 | loss: 2.7270 | lr: 0.000100\nEpoch 51/70 | loss: 2.6975 | lr: 0.000100\nEpoch 52/70 | loss: 2.6539 | lr: 0.000100\nEpoch 53/70 | loss: 2.6364 | lr: 0.000100\nEpoch 54/70 | loss: 2.5988 | lr: 0.000100\nEpoch 55/70 | loss: 2.5718 | lr: 0.000100\nEpoch 56/70 | loss: 2.5543 | lr: 0.000100\nEpoch 57/70 | loss: 2.5122 | lr: 0.000100\nEpoch 58/70 | loss: 2.4894 | lr: 0.000100\nEpoch 59/70 | loss: 2.4647 | lr: 0.000100\nEpoch 60/70 | loss: 2.4353 | lr: 0.000100\nEpoch 61/70 | loss: 2.4002 | lr: 0.000100\nEpoch 62/70 | loss: 2.3706 | lr: 0.000100\nEpoch 63/70 | loss: 2.3445 | lr: 0.000100\nEpoch 64/70 | loss: 2.3167 | lr: 0.000100\nEpoch 65/70 | loss: 2.2953 | lr: 0.000100\nEpoch 66/70 | loss: 2.2727 | lr: 0.000100\nEpoch 67/70 | loss: 2.2355 | lr: 0.000100\nEpoch 68/70 | loss: 2.2013 | lr: 0.000100\nEpoch 69/70 | loss: 2.1712 | lr: 0.000100\nEpoch 70/70 | loss: 2.1602 | lr: 0.000100\n\nTraining Hindi model...\ntraining started : \ndevice: cuda\nEpoch 1/70 | loss: 5.7292 | lr: 0.000100\nEpoch 2/70 | loss: 5.0423 | lr: 0.000100\nEpoch 3/70 | loss: 4.7828 | lr: 0.000100\nEpoch 4/70 | loss: 4.6264 | lr: 0.000100\nEpoch 5/70 | loss: 4.4904 | lr: 0.000100\nEpoch 6/70 | loss: 4.3621 | lr: 0.000100\nEpoch 7/70 | loss: 4.2692 | lr: 0.000100\nEpoch 8/70 | loss: 4.1662 | lr: 0.000100\nEpoch 9/70 | loss: 4.0895 | lr: 0.000100\nEpoch 10/70 | loss: 4.0151 | lr: 0.000100\nEpoch 11/70 | loss: 3.9567 | lr: 0.000100\nEpoch 12/70 | loss: 3.8828 | lr: 0.000100\nEpoch 13/70 | loss: 3.8163 | lr: 0.000100\nEpoch 14/70 | loss: 3.7587 | lr: 0.000100\nEpoch 15/70 | loss: 3.7164 | lr: 0.000100\nEpoch 16/70 | loss: 3.6421 | lr: 0.000100\nEpoch 17/70 | loss: 3.5826 | lr: 0.000100\nEpoch 18/70 | loss: 3.5440 | lr: 0.000100\nEpoch 19/70 | loss: 3.4855 | lr: 0.000100\nEpoch 20/70 | loss: 3.4285 | lr: 0.000100\nEpoch 21/70 | loss: 3.3840 | lr: 0.000100\nEpoch 22/70 | loss: 3.3351 | lr: 0.000100\nEpoch 23/70 | loss: 3.2907 | lr: 0.000100\nEpoch 24/70 | loss: 3.2472 | lr: 0.000100\nEpoch 25/70 | loss: 3.2033 | lr: 0.000100\nEpoch 26/70 | loss: 3.1524 | lr: 0.000100\nEpoch 27/70 | loss: 3.1058 | lr: 0.000100\nEpoch 28/70 | loss: 3.0733 | lr: 0.000100\nEpoch 29/70 | loss: 3.0304 | lr: 0.000100\nEpoch 30/70 | loss: 2.9829 | lr: 0.000100\nEpoch 31/70 | loss: 2.9326 | lr: 0.000100\nEpoch 32/70 | loss: 2.9077 | lr: 0.000100\nEpoch 33/70 | loss: 2.8622 | lr: 0.000100\nEpoch 34/70 | loss: 2.8147 | lr: 0.000100\nEpoch 35/70 | loss: 2.7888 | lr: 0.000100\nEpoch 36/70 | loss: 2.7469 | lr: 0.000100\nEpoch 37/70 | loss: 2.7026 | lr: 0.000100\nEpoch 38/70 | loss: 2.6573 | lr: 0.000100\nEpoch 39/70 | loss: 2.6197 | lr: 0.000100\nEpoch 40/70 | loss: 2.5921 | lr: 0.000100\nEpoch 41/70 | loss: 2.5588 | lr: 0.000100\nEpoch 42/70 | loss: 2.5227 | lr: 0.000100\nEpoch 43/70 | loss: 2.4823 | lr: 0.000100\nEpoch 44/70 | loss: 2.4477 | lr: 0.000100\nEpoch 45/70 | loss: 2.4201 | lr: 0.000100\nEpoch 46/70 | loss: 2.3761 | lr: 0.000100\nEpoch 47/70 | loss: 2.3492 | lr: 0.000100\nEpoch 48/70 | loss: 2.3097 | lr: 0.000100\nEpoch 49/70 | loss: 2.2992 | lr: 0.000100\nEpoch 50/70 | loss: 2.2519 | lr: 0.000100\nEpoch 51/70 | loss: 2.2171 | lr: 0.000100\nEpoch 52/70 | loss: 2.1835 | lr: 0.000100\nEpoch 53/70 | loss: 2.1673 | lr: 0.000100\nEpoch 54/70 | loss: 2.1399 | lr: 0.000100\nEpoch 55/70 | loss: 2.0887 | lr: 0.000100\nEpoch 56/70 | loss: 2.0746 | lr: 0.000100\nEpoch 57/70 | loss: 2.0375 | lr: 0.000100\nEpoch 58/70 | loss: 2.0099 | lr: 0.000100\nEpoch 59/70 | loss: 1.9819 | lr: 0.000100\nEpoch 60/70 | loss: 1.9491 | lr: 0.000100\nEpoch 61/70 | loss: 1.9210 | lr: 0.000100\nEpoch 62/70 | loss: 1.8959 | lr: 0.000100\nEpoch 63/70 | loss: 1.8748 | lr: 0.000100\nEpoch 64/70 | loss: 1.8521 | lr: 0.000100\nEpoch 65/70 | loss: 1.8142 | lr: 0.000100\nEpoch 66/70 | loss: 1.7948 | lr: 0.000100\nEpoch 67/70 | loss: 1.7686 | lr: 0.000100\nEpoch 68/70 | loss: 1.7311 | lr: 0.000100\nEpoch 69/70 | loss: 1.7112 | lr: 0.000100\nEpoch 70/70 | loss: 1.6974 | lr: 0.000100\n\nGenerating Bengali translations...\n✅ Saved Bengali predictions → answer_ben.csv\n\nGenerating Hindi translations...\n✅ Saved Hindi predictions → answer_hi.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"id":"210TJunwCkIC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nCreating final submission file\")\nimport pandas as pd\ndf_ben = pd.read_csv(\"/kaggle/input/erfiof\")\ndf_hin = pd.read_csv(\"/kaggle/input/fbthth\")\n\n# Combine Bengali first, then Hindi\ncombined_data = pd.concat([df_ben, df_hin], axis=0, ignore_index=True)\n\n# Save in the exact format required\nwith open(\"/kaggle/working/answer.csv\", \"w\") as f:\n    f.write(\"ID\\tTranslation\\n\")\n    for i in range(combined_data.shape[0]):\n        f.write(f\"{combined_data['ID'][i]}\\t{combined_data['Translation'][i]}\\n\")\n\nprint(\"Final submission file created → answer.csv\")\nprint(f\"Total rows: {combined_data.shape[0] + 1} (including header)\")  # +1 for header\nprint(f\"Bengali entries: {len(df_ben)}\")\nprint(f\"Hindi entries: {len(df_hin)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_1CiPwAFxvE","outputId":"1a930fe7-5475-4863-bc88-9ca2b2c205a8","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:48:57.770697Z","iopub.execute_input":"2025-10-23T18:48:57.771092Z","iopub.status.idle":"2025-10-23T18:48:57.793649Z","shell.execute_reply.started":"2025-10-23T18:48:57.771056Z","shell.execute_reply":"2025-10-23T18:48:57.791976Z"}},"outputs":[{"name":"stdout","text":"\nCreating final submission file\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1845847632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCreating final submission file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_ben\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/erfiof\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_hin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/fbthth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/kaggle/input/erfiof'"],"ename":"IsADirectoryError","evalue":"[Errno 21] Is a directory: '/kaggle/input/erfiof'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"id":"7hL3Ot2nvG6D","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"hhu599TniH57","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"YMomXuZ9aT3r","trusted":true},"outputs":[],"execution_count":null}]}